\documentclass{article}
\usepackage{eigilscmds}
\title{Category theory for Statistics}
\author{Eigil Fjeldgren Rischel}
\date{}

\begin{document}
\maketitle
\section{Introduction}

The goal of this note is to introduce certain basic concepts of category theory, in a context which will hopefuly make them understandable to people with a background in statistics or similar fields.

There are already several excellent books introducing category theory at an elementary level. I will mention, one, Riehl's \emph{Categories in Context}.
It is an excellent supplement or replacement for these notes, and available for free on the author's webpage.

The guiding philosophy of category theory is that, when trying to understand a mathematical object, it is usually extremely useful to think about the right notion of \emph{morphism}, i.e structure-preserving transformation, between this type of objects.
To this we can perhaps add another idea, that one should understand complicated things by understanding how they are composed of simpler things.

I will try to describe several ways to bring these viewpoints to bear on questions of statistics. First, we will take \emph{Markov kernels} as the notion of transformation between measurable spaces, and try to do statistics in this context.
An interesting fact about this approach is that many definitions and results in statistics fall out of the ``abstract structure'' of this setting, and are thus in a sense independent of measure theory.

\section{Markov kernels}
Recall the definition of a \emph{Markov kernel}
\todo{definition of markov kernel}

(Should we work with stochastic matrices first?)

Given markov kernels $A \labelto{f} B \labelto{g} C$, we can form their \emph{composition} $g\circ f: A \to C$.
If we think of a Markov kernel as a map involving some randomness, this composition simply gives the composite map, under the assumption that the two component maps use independent sources of randomness.
If $X, Y, Z$ are stochastic variables with values in $A,B,C$, so that $f$ is the conditional distribution of $Y$ given $X$, and $g$ is the conditional distribution of $Z$ given $Y$, then $g\circ f$ is the conditional distribution of $Z$ given $X$, if and only if $Z$ is conditionally independent of $X$ given $Y$. (We will give a neat proof of this later)
\todo{definition of composition}

A few more constructions are important: given $f:A \to B$ and $g:X \to Y$ Markov kernels, we can form the ``parallel composition'' (we might call the previous type "sequential" composition) $A \times X \to B \times Y$,
given essentially by $P(b,y|a,x) = P(b|a)P(y|x)$. We write this as $f \tensor g$. To harmonize the notation, we will also write the product measure space as $A \tensor B$, rather than $A \times B$.


We can depict Markov kernels in diagrams like this:

\todo{examples of diagrams}

The meaning of these is probably quite intuitive, but let us spell it out

\begin{enumerate}
    \item A line represents a measurable space. A collection of lines drawn next to each other represent the product of those spaces. 
    \item Each labeled box represents a Markov kernel.
    \item The diagram obtained by putting Markov kernels on top of each other and wiring them together represents the (sequential) composition.
    \item The diagram obtained by putting kernels next to each other represents the parallel composition.\footnotemark
\end{enumerate}

\footnote{There is an issue here regarding associativity - because the spaces $(A \tensor B) \tensor C$ and $A \tensor (B \tensor C)$ are not \emph{equal}, there is a question of which order to apply the parallel composition in. This turns out to not really matter - we can abuse notation and identify these two sets without worrying about it}

Using this, we can break down a diagram like (ref)\todo{fix} into $(f \tensor g) \circ h$.
To break down (ref)\todo{fix - one with identities} we need to add the convention that an undecorated line stands for the \emph{identity kernel} $X \to X$ which sends $x\in X$ to the dirac measure $\delta_x$.
This choice is not arbitrary - to make sure that each diagram represents the same kernel, no matter how we break it down, the kernel $f_X$ represented by the line needs to satisfy $f_Yg = g = gf_X$ for all kernels $g$.
It's clear that any kernel which satisfies this is equal to the identity kernel.
We write the identity kernel $1_X$.

Let us introduce a few more pieces of special notation.

\begin{example}
    Suppose we have $P: I \to (A \tensor B \tensor C)$ representing the joint distribution of three random variables $(X,Y,Z)$.
    Suppose $P$ factors like this:
    \todo{diagram involving $f: A \to B$ and $g: B \to C$}
    Then $X$ is independent of $Z$ given $Y$.
    To see this, consider this diagram.
    Since the conditional probability of $X$ given $Y$ exists, we can rewrite to this diagram:
    Now for $P$-almost all $y\in B$, the conditional distribution of $(X,Z)$ is a product distribution, which is what we wanted.
\end{example}
\end{document}