\documentclass{article}
\usepackage{eigilscmds}
\usepackage{tikzit}

\usepackage{bbm}
\input{foo.tikzstyles}
\newcommand{\del}{\mathsf{del}}
\newcommand{\cop}{\mathsf{copy}}
\DeclareMathOperator{\ob}{ob}
\DeclareMathOperator{\Grp}{Grp}
\DeclareMathOperator{\Met}{Met}
\DeclareMathOperator{\FdVect}{FdVect}
\DeclareMathOperator{\Stoch}{Stoch}
\DeclareMathOperator{\Prob}{Prob}
\DeclareMathOperator{\Gauss}{Gauss}
\DeclareMathOperator{\FinStoch}{FinStoch}
\DeclareMathOperator{\Triv}{Triv}
\usepackage{cleveref}
\title{Category theory for Statistics}
\author{Eigil Fjeldgren Rischel}
\date{}

\begin{document}
\maketitle
\section{Introduction}

The goal of this note is to introduce certain basic concepts of category theory, in a context which will hopefuly make them understandable to people with a background in statistics or similar fields.

There are already several excellent books introducing category theory at an elementary level. I will mention, one, Riehl's \emph{Categories in Context}.
It is an excellent supplement or replacement for these notes, and available for free on the author's webpage.
There is also the wonderful book \emph{Seven Sketches in Compositionality} by Brendan Fong and David Spivak.
It's less of a compendium of category theory, and more of a motivating introduction, written from the viewpoint of applied mathematics.
I would definitely recommend looking at \emph{Seven Sketches} before looking at these notes.
(Like \emph{Categories in Context}, it can be found on arXiv).

The guiding philosophy of category theory is that, when trying to understand a mathematical object, it is usually extremely useful to think about the right notion of \emph{morphism}, i.e structure-preserving transformation, between this type of objects.
To this we can perhaps add another idea, that one should understand complicated things by understanding how they are composed of simpler things.

I will try to describe several ways to bring these viewpoints to bear on questions of statistics. First, we will take \emph{Markov kernels} as the notion of transformation between measurable spaces, and try to do statistics in this context.
An interesting fact about this approach is that many definitions and results in statistics fall out of the ``abstract structure'' of this setting, and are thus in a sense independent of measure theory.

I will generally skip over technical details about measure theory without much comment or reference. \todo{flesh}
\section{Markov kernels}
Recall the definition of a \emph{Markov kernel}
\begin{definition}
    If $(A,\Sigma), (B,\Gamma)$ are measurable spaces, a \emph{Markov kernel} $A \to B$ is given by a map $f: A \times \Gamma \to [0,1]$ such that
    \begin{itemize}
        \item For each $a\in A$, $f(a,-)$ is a probability measure on $B$.
        \item For each $U \in \Gamma$, $f(-,U): A \to [0,1]$ is a measurable function.
    \end{itemize}
\end{definition}
Markov kernels can be conceptualized as ``maps with randomness'', or perhaps as conditional probability distributions.

\begin{definition}
Given markov kernels $A \labelto{f} B \labelto{g} C$, we can form their \emph{composition} $g\circ f: A \to C$.
Is is given by $(g\circ f)(a)(U) = \bE_{b \sim f(a)}g(b)(U)$.
In other words, it is the average probability, according to $g(b)$, of $U$, if $b$ is distributed according to $f(a)$.
\end{definition}
(Of course, it must be verified that this is a Markov kernel)

If we think of a Markov kernel as a map involving some randomness, this composition simply gives the composite map, under the assumption that the two component maps use independent sources of randomness.
If we instead think of a Markov kernel as a conditional distribution, then the composition is given by computing the composed conditional distribution under the assumption of conditional independence.
If $X, Y, Z$ are stochastic variables with values in $A,B,C$, so that $f$ is the conditional distribution of $Y$ given $X$, and $g$ is the conditional distribution of $Z$ given $Y$, then $g\circ f$ is the conditional distribution of $Z$ given $X$, if and only if $Z$ is conditionally independent of $X$ given $Y$. (We will give a neat proof of this later)
A few more constructions are important: given $f:A \to B$ and $g:X \to Y$ Markov kernels, we can form the ``parallel composition'' (we might call the previous type "sequential" composition) $A \times X \to B \times Y$,
given essentially by $P(b,y|a,x) = P(b|a)P(y|x)$. We write this as $f \tensor g$. To harmonize the notation, we will also write the product measure space as $A \tensor B$, rather than $A \times B$.


We can depict Markov kernels in diagrams like this:
\todo{examples of diagrams}

The meaning of these is probably quite intuitive, but let us spell it out

\begin{enumerate}
    \item A line represents a measurable space. A collection of lines drawn next to each other represent the product of those spaces. 
    \item Each labeled box represents a Markov kernel.
    \item The diagram obtained by putting Markov kernels on top of each other and wiring them together represents the (sequential) composition.
    \item The diagram obtained by putting kernels next to each other represents the parallel composition.\footnotemark
\end{enumerate}

\footnote{There is an issue here regarding associativity - because the spaces $(A \tensor B) \tensor C$ and $A \tensor (B \tensor C)$ are not \emph{equal}, there is a question of which order to apply the parallel composition in. This turns out to not really matter - we can abuse notation and identify these two sets without worrying about it}

\begin{figure}
    \begin{center}
    \input{exdiag1.tikz}
    \end{center}
\caption{$(f \tensor g) \circ h$}
    \label{fig:ex1}
\end{figure}
    
\begin{figure}
\begin{center}
\input{exdiag2.tikz}
\end{center}
\caption{$(1_X \tensor f) \circ g$}
\label{fig:ex2}
\end{figure}

Using this, we can break down a diagram like \cref{fig:ex1} $(f \tensor g) \circ h$.
To break down \cref{fig:ex2} we need to add the convention that an undecorated line stands for the \emph{identity kernel} $X \to X$ which sends $x\in X$ to the dirac measure $\delta_x$.
This choice is not arbitrary - to make sure that each diagram represents the same kernel, no matter how we break it down, the kernel $f_X$ represented by the line needs to satisfy $f_Yg = g = gf_X$ for all kernels $g$.
It's clear that any kernel which satisfies this is equal to the identity kernel.
We write the identity kernel $1_X$.

\begin{figure}
\begin{center}
\input{cross.tikz}
\end{center}
\caption{Crossed wires - a kernel $X \tensor Y \tensor Z \to W$}
\label{fig:cross}
\end{figure}


To break down \cref{fig:cross}, we need to add another convention, namely that a pair of crossed wires represents the kernel $X \tensor Y \to Y \tensor X$
which sends $(x,y)$ to the dirac measure $\delta_{(y,x)}$.
If we need notation for this kernel, we write it as $\sigma_{X,Y}$.

Observe that a kernel which sends each point to a dirac measure is essentially the same thing as a (measurable) function, viewed as a stochastic process which is not really stochastic at all, but deterministic. In this sense the identity kernel ``is just'' the identity function, and $\sigma_{X,Y}$ ``is'' the map swapping the coordinates.

\begin{figure}
\begin{center}
\input{copyX.tikz}
\end{center}
\caption{$\cop_X$}
\label{fig:copy}
\end{figure}

\begin{figure}
\begin{center}
\input{deleteX.tikz}
\end{center}
\caption{$\del_X$}
\label{fig:delete}
\end{figure}

Let us introduce a few more pieces of special notation.

\begin{enumerate}
    \item For each $X$ there is a kernel $X \to X \tensor X$ sending $x \in X$ to the dirac measure $\delta_{(x,x)}$.
    We write this as $\mathsf{copy}_X: X \to X \tensor X$ and draw it as a dot, like \cref{fig:copy}. This corresponds to the diagonal map $X \to X \tensor X$ in the sense discussed above.
    \item Also for each $X$, there is a unique kernel $X \to 1$, where $1$ is the one-point space (since there is only one probability measure on this space).
    We write this as $\mathsf{del}_X: X \to 1$, and draw it as a dot with nothing coming out of it, like \cref{fig:delete}. This also corresponds to the unique deterministic map $X \to 1$.
\end{enumerate}

To facilitate (2), we need to add the convention that the lack of a string can represent $1$.
This essentially means we're using the identification $X \times 1 = X$ - just like the previous footnote about associativity, this is harmless.
A kernel $1 \to X$ is just the same thing as a probability distribution on $X$. We draw these as triangles, like\todo{figure}.

We now have a rich graphical language for drawing Markov kernels.
It turns out that a lot of questions that are important in statistics can be phrased as questions about diagrams.
\begin{figure}
    \begin{center}
    \input{cond.tikz}
    \end{center}
    \caption{Conditional probability}
    
    \label{fig:conditional}
\end{figure}   

\begin{example}
    Consider two random variables $X\in A,Y\in B$with join distribution $P: 1 \to A \tensor B$. Then:
    \begin{enumerate}
        \item $X$ is independent of $Y$ if and only if these two diagrams represent the same kernel.
        \item $f:A \to B$ is the (a) conditional probability of $Y$ given $X$ if and only if the two diagrams in \cref{fig:conditional} represent the same kernel.
    \end{enumerate}
    We can combine these:
    \begin{enumerate}
        \item[3] Given three random variables with joint distribution $P: 1 \to X \tensor Y \tensor Z$, $X$ and $Y$ are conditionally independent given $Z$ if and only if there is a diagram of the depicted form equal to the first one. \todo{awkward formulation}.
        To see this, first note that a diagram of this form\todo{new diagram} equal to the original one gives a conditional distribution of $X,Y$ given $Z$.
        Then they are independent if and only if this is a product distribution for ($P$-almost) all $z\in C$, if and only if the depicted equality holds.
    \end{enumerate}
\label{ex:indeps}
\end{example}
\begin{example}
    Suppose we have $P: I \to (A \tensor B \tensor C)$ representing the joint distribution of three random variables $(X,Y,Z)$.
    Suppose $P$ factors like this:
    \todo{diagram involving $f: A \to B$ and $g: B \to C$}
    Then $X$ is independent of $Z$ given $Y$.
    To see this, consider this diagram.
    Since the conditional probability of $X$ given $Y$ exists, we can rewrite to this diagram:
    Which is precisely what we wanted, by \cref{ex:indeps} above.
\end{example}

\todo{some worked examples}

\section{Worked example: Causal inference}
(This section essentially stolen from \cite{CausalSurgery}).

Now I'll use string diagrams to do a simple statistical problem: inferring an interventional distribution from an observed distribution.
The statistics of this section is probably familiar to statisticians, but I will give some exposition anyway, to make sure everyone is on the same page (since it's quite new to me).

Consider the following classical inference problem: we have two observed variables,
$X$ and $Y$. We observe a correlation between these two variables, but we are unsure about whether this connection is causal.
In other words, we want to know if it will persist if we intervene on $X$.
This is not, in general, possible to do just using observational data.
Using only measurements of $X$ and $Y$, we cannot distinguish between the possibilities
``$X$ causes $Y$'', ``$Y$ causes $X$'', and ``$X$ and $Y$ share an unobserved common cause''.

To make things concrete, let's say $X$ is the binary variable measuring whether a person smokes, and $Y$ is the binary variable measuring whether a person has lung cancer.
In this case, the observed correlation is extremely strong.
Moreover, the question of causality is very important - if smoking does cause cancer (and is not simply caused by some other factor which also causes cancer), we want to know about it!
We are trying to distinguish between the two causal graphs shown in figure \todo{add figure}, with the caveat that we cannot measure $F$.

The trick to doing this is to add some domain knowledge - we actually think we know \emph{how} smoking causes cancer, namely by adding tar to the lungs. So we add a tar node to our diagram. \todo{figure}.
Now it's actually possible to figure this out!
The intuitive reason is that, if there's a common cause of smoke and cancer, then smoking and cancer should still be correlated even conditioning on the presence of tar in the lungs.
But if not, then conditioning on the presence of tar should render smoking and cancer independent.


We model this in the following way: \todo{figure!}
All our variables are binary (for simplicity). They are 
\begin{enumerate}
    \item $F$, a hypothetical common cause of smoking and cancer.
    \item $S$, smoking/non-smoking.
    \item $C$, cancer/no cancer.
    \item $T$, tar/no tar.
\end{enumerate}
We suppose that there is a background distribution $P: 1 \to F$,
kernels $f: F \to S, g: S \to T, h: T \tensor F \to C$, which describe the conditional distribution of each variable given its causes. \todo{diagram}.

Then the \emph{observed distribution} is the distribution
\[(1_T \tensor h)\circ (\cop_Tgf\tensor 1_F )\circ\cop_F\circ P,\]
which is depicted in \todo{diagram}.
Let's make up a sample


Now we want to understand the \emph{interventional distribution}.
To be concrete, let's ask this question: if we assign each person to smoke or not smoke at random (with equal probability), what is the distribution of $(S,C)$? How strong is the correlation?

Letting $U: 1 \to S$ be the uniform distribution, this distribution is given by \todo{figure}.
The surprise is that this can actually be calculated from the observational distribution.
To accomplish this, first we take the conditional probability of tar given smoking, and the conditional probability of cancer given tar and smoking (in terms of the observational distribution).
This gives us a diagram \todo{make it}.
In fact, we can fill in these boxes, obtaining \todo{rewritten figure}.
But observe that the dashed box is precisely the conditional probability - in other words, it is determined from the observation distribution.
Filling it into our interventional diagram, we obtain \todo{diagram} , which we can calculate - the marginal of $S$ is uniform, the conditional distribution of $T$ given $S$ is just $g$, and the conditional of $C$ given $T$ is precisely $\int_f h(t,f) dP(f)$.


I hope this example illustrates how manipulating string diagrams can make some arguments more easy to understand.


\section{Abstract structure}
Hopefully by now I've given you some idea that these diagrams are a nice way of working with random variables.
Now I'm going to pull the rug out and reveal that we've secretly been working with category theory the whole time.
The structure of the diagrams we have been working with corresponds exactly to the structure of a certain type of category,
known by several names in the literature. I am going to call them Markov categories, a name introduced by Tobias Fritz in \todo{ref!}.
It's a great reference, which I have largely been cribbing for these notes so far, although it may not be very readable without a background in category theory.

Let us ask this question: if we wanted to make sense of the diagrams we have been using so far, but we didn't know anything about the underlying objects - didn't know that the labels on the lines correspond to measurable spaces, or that the boxes correspond to kernels, or anything like that - what is the bare minimum of structure we need to intepret these diagrams?
This may seem like an extremely odd question, but in fact it is extremely natural from the point of view of category theory.

\begin{enumerate}
    \item Certainly there is a set of possible labels for the lines (e.g. measurable spaces)\footnotemark, which we could maybe just call the set of \emph{labels}.
    \item There is also a set of possible ``connections'', e.g. kernels, which are the things that each diagram can be interpreted as. Let us just call these the set of \emph{diagrams} (although there is a subtle issue with this - graphically distinct diagrams may depict the same kernel).
    \item Each diagram has a certain collection of labels as its beginning, and another one as its end. We can write this symbolically for now as
    \[f: x_1, x_2, \dots x_m \to y_1, y_2, \dots, y_n\]
    \item If the labels at the beginning of a diagram agree with the labels at the end of another diagram, we can put the diagrams on top of one another, \emph{composing} them.
    Write this as functional composition, i.e:
    \[f: x_1, x_2, \dots x_m \to y_1, y_2, \dots y_n, g:y_1, \dots y_n \to z_1, \dots z_k\]
    \[g\circ f : x_1, \dots x_m \to z_1, \dots z_k\]
    \item We can also put diagrams next to each other (no matter what the labels are) - in that case, we also concatenate the lists of labels. Write this using $\tensor$.
    \item For each label, there are special diagrams $\cop_X: X \to X,X$ and $\del_X: X \to$ (meaning $\del$ has an \emph{empty} list of output labels).
    \item There is a special diagram $1_X : X \to X$, and a special diagrams $\sigma_{X,Y}: X,Y \to Y,X$.
\end{enumerate}
\footnotemark{Of course, there is not really a \emph{set} of measurable spaces, since this would entail a set of all sets.. we can ignore this issue for now, or suppose that for the first part we worked only with a restricted class of measurable spaces.}

The structure sketched above corresponds roughly to what category theorists call a \emph{colored PROP} (although of course I have not been completely precise).
The only exception is 5., which turns out to be someone special for probability theory. We will meet it again later.
In fact, we have a bit of extra structure, which we have not really made visible in the diagrams yet (and which in fact doesn't really have a consistent notation).
This is because kernels $A,B \to C$ are actually kernels $A \tensor B \to C$ - diagrams can always be considered as having a single source and target.
Thus we can add these bits of structure:
\begin{enumerate}
    \item For each pair of labels $x,y$, there is a diagram $f: x,y \to x\tensor y$ with the following property: composing with $f$ gives a bijection between diagrams $x \tensor y \to z_1, \dots z_n$ and diagrams $x,y \to z_1, \dots z_n$, for all sequences $z_1, \dots z_n$ of labels.
    \item Ther is a label $I$ and a diagram $f: \to I$ with the following property: composing with $f$ gives a bijection between diagrams $I \to z_1, \dots z_n$ and diagrams $\to z_1, \dots z_n$, for all sequences of labels.
\end{enumerate}
This makes our structure into a \emph{representable} colored PROP.
With this, we don't actually need to discuss the maps with multiple inputs and outputs - they are completely described by describing single input/output maps.
This is in many ways more natural - we don't usually speak of a map with two inputs $A, B \to C$, but just of a map $A \times B \to C$, which we may think of as having one or two inputs.
In the history of category theory, that's how it went, too. People first developed the notion of \emph{monoidal} category that I'm about to describe, and only later came up with representable PROPs (first, of course, they came up with PROPs, which are interesting in their own right).
The reason I developed things in this rather silly order is because string diagrams are more naturally interpreted in PROPs.

We will approach the definition of ``monoidal category'' in several steps, roughly corresponding to the order above.
Along the way, we will need to introduce several other notions, in order to make the definition manageable.
I will try to motivate these definitions - far from being simply utilities to make the main definition easier, they are really the bread and butter of category theory.

\begin{definition}
    A \emph{category} $\cC$ consists of the following data.
    \begin{enumerate}
        \item A set of \emph{objects}, $\ob \cC$.
        \item For each $X,Y \in \ob \cC$, a set $\cC(X,Y)$ of \emph{morphisms $X \to Y$}. If $f\in \cC(X,Y)$ we write $f: X \to Y$.
        \item For each $X,Y,Z \in \ob \cC$, a \emph{composition} $\cC(Y,Z) \times \cC(X,Y) \to \cC(X,Z)$
        \item For each $X \in \ob \cC$, an \emph{identity} $1_X \in \cC(X,X)$.
    \end{enumerate}
    Satisfying $1_X \circ f = f = 1_Y \circ f$ for $f: X \to Y$, and $(f\circ g)\circ h = f \circ (g \circ h)$.
\end{definition}
One usually simply writes $X \in \cC$ when $X$ is an object of $\cC$, for brevity.
\begin{example}
    Some examples of categories are
    \begin{enumerate}
        \item The category $\Set$ where objects are sets, morphisms are functions, and composition is composition of functions.
        \item The category $\Meas$ where objects are measurable spaces, morphisms are measurable functions, and composition is again simply composition of functions.
        \item The category $\Stoch$ where objects are measurable spaces, morphisms are \emph{Markov kernels}, and composition is as above.
        \item The category $\Prob$ where objects are triples $(X,\Sigma,P)$ s.t. $(X,\Sigma)$ is a measurable space and $P$ is a probability measure on it, and morphisms are measurable, measure-preserving maps.
    \end{enumerate}
\end{example}
These examples illustrate a common problem: a category is not uniquely determined by its objects, so $f:X \to Y$ is ambiguous - if $X,Y$ are measurable spaces, $f$ could be a Markov kernel or a measurable map. This is usually taken to be clear from context, or one can write $f:X \to Y \in \Stoch$ for clarity.
Also, most classes of objects do have a ``natural'' notion of morphisms, so that $\Meas$ may be called ``the category of measurable spaces'' without too much worry.

Categories are the structure necessary to implement \emph{sequential} composition, i.e diagrams like \todo{diagram}.
Of course, diagrams like that are not very intereting, so string diagrams are not very useful for categories unless they have extra structure.
When working in a mere category, it is more common to use \emph{commutative diagrams} like this

\begin{tikzcd}
\bR \ar[r, "{x \mapsto (x,x)}"] \ar[rd, swap, "{x \mapsto 0}"] & \bR \times \bR \ar[d, "{(x,y) \mapsto x -y}"]\\
& \bR
\end{tikzcd}

Which express the equality of two different maps (in this case, of the constant $0$ map and the composition depicted).

\begin{example}
    There are many more examples of categories:
    \begin{enumerate}
        \item $\FdVect$: Objects are finite-dimensional (real) vector spaces, maps are linear maps.
        \item $\Grp$: Objects are groups, maps are group homomorphisms.
        \item $\Top$: Objects are topological spaces, maps are continuous maps.
        \item $\Met$: Objects are metric spaces, maps are \emph{short} maps, i.e those satisfying $d_Y(f(x_1),f(x_2)) \leq d_X(x_1,x_2)$.
        \item $\Ab$: Objects are abelian groups, maps are again group homomorphisms.
    \end{enumerate}
    There are also more exotic examples:
    \begin{enumerate}
    \item If $(X,\leq)$ is a partially ordered set, there is a category $O(X)$ with $\ob O(X) = X$ and $O(X)(a,b) = \begin{cases} \{*\} & a \leq b\\\emptyset & \text{else}\end{cases}$.
    In other words, there is a unique map $a \to b$ if $a \leq b$, and no maps otherwise. Verifying that this defines a category is a good exercise.
    \item If $G$ is a group, there is a category $BG$ with one object $*$, $BG(*,*) = G$, and composition defined by the group multiplication.
    \end{enumerate}
\end{example}

Having immersed ourselves in theory for a while, let's return to our example of Markov kernels for a while.
Recall the special kernel $1_X: X \to X$, which is defined by $1_X(x,U) = \mathbbm{1}_U(x)$, \todo{figure out notation} i.e $1$ if $x \in U$ and $0$ else.
In the interpretation of a Markov kernel as a ``random map'', this kernel corresponds to the map which non-randomly maps each element to itself.
We can define a large class of Markov kernels in this way:
\begin{definition}
    Given a measurable function $f: X \to Y$, the induced markov kernel $\bar{f}: X \to Y$ is defined as $\bar{f}(x,U) = \mathbbm{1}_U(f(x))$.
\end{definition}
An important property of this construction is that it's \emph{compatible with composition}, in the sense that $\overline{f \circ g} = \bar{f} \circ \bar{g}$,
when $f,g$ are composable measurable functions.

\begin{definition}
    Let $\cC, \cD$ be categories. A \emph{functor} $F: \cC \to \cD$ consists of
    \begin{enumerate}
        \item A map $F: \ob \cC \to \ob \cD$.
        \item For each $X,Y \in \cC$, a map $F: \cC(X,Y) \to \cD(FX,FY)$.
        \item So that $F(f\circ g) = F(f) \circ F(g)$, and $F(1_X) = 1_{FX}$. (We say the functor preserves the composition and the units).
    \end{enumerate}
\end{definition}
As you can see, we usually just use the symbol $F$ for all the various maps involved in a functor. We also often omit the parentheses.

\begin{example}
    \begin{enumerate}
    \item The assignment $X \mapsto X$ for a measurable space $X$, and $f: X \to Y \mapsto ((x,U) \mapsto x \in U)$ defines a functor $\Meas \to \Stoch$,
    which sends measurable a space to itself, and each measurable map to the corresponding deterministic Markov kernel (where $x$ goes to the Dirac measure $\delta_{f(x)}$).
    Verifying that this is actually a functor is an instructive exercise.
    \item For any category $\cC$, there is an ``identity functor'' $1_\cC: \cC \to \cC$.
    \item There is a functor $\Triv: \Set \to \Meas$ which sends each set $X$ to $(X,\cP(X))$, and each function $f: X \to Y$ to itself.
    \item There is a functor $U: \Meas \to \Set$ which sends each measurable space $(X,\Sigma)$ to the underlying set $X$, and each measurable function to itself.
    \item There is a functor $U: \Prob \to \Meas$ which simply ``forgets'' the probability distributions.
    \item There is a functor $\Met \to \Top$ which sends each metric space to the underlying set in the induced topology.
    \item There is a functor $\Top \to \Meas$ which sends each space to the uderlying set in the Borel $\sigma$-algebra.
    \item A functor $BG \to \Set$ is precisely a set with a $G$-action. A functor $BG \to \FdVect$ is precisely a finite-dimensional (real) representation of $G$.
    \item A functor $(\bN,\leq) \to \Stoch$ consists of a sequence of measurable spaces $X_1, \dots$, and transition kernels $X_i \to X_{i+1}$. We may think of this as a discrete-time stochastic process (with possibly changing state space).
    \item Similarly, a functor $(\bR,\leq) \to \Stoch$ consists of measurable spaces $\{X_r \mid r \in \bR\}$,
    with a kernel $t_{rs}: X_r \to X_s$ for each $r \leq s$, s.t $t_{rt} = t_{st}t_{rs}$.
    This can be viewed as a continuous-time stochastic process (although one without any sort of continuity properties).
    \end{enumerate}
\end{example}
The two functors both named $U$ are called \emph{forgetful functors}, because they ``forget'' some of the structure involved (the first one forgets the $\sigma$-algebra).
Such functors are often denoted $U$ - I am not sure why, but I suppose it might be short for ``underlying''.


Functors preserve diagrams. What I mean by this is that applying a functor to a commutative diagram like \todo{ref} will result in another commutative diagram.
And in fact this equivalently characterizes functors.
Hence \emph{functors are those mappings which are compatible with diagrammatic reasoning}.
When we later see (symmetric) monoidal categories and (symmetric) monoidal functors, we will see that they are precisely those which are compatible with string diagrams.

\begin{definition}
The category $\Cat$ has
\begin{enumerate}
    \item Objects all categories
    \item Morphisms $\cC \to \cD$ simply the functors $\cC \to \cD$.
    \item Composition defined the obvious way, identities as defined above.
\end{enumerate}
\end{definition}

\begin{definition}
A \emph{monoidal category} is a triple $(\cC, \otimes, I, \alpha, \lambda, \rho)$ where
    \begin{enumerate}
        \item $\cC$ is a category.
        \item $\otimes$ is a functor $\cC \times \cC \to \cC$, called the \emph{monoidal product}.
        \item $I \in \cC$ is an object, called the \emph{unit}.
        \item $\lambda$ is a natural isomorphism $\lambda_X: 1 \tensor X \labelto{\sim} X$.
        \item $\rho$ is a natural isomorphism $\rho_X: X \otimes 1 \labelto{\sim} X$.
        \item $\alpha$ is a natural isomorphism $\alpha_{X,Y,Z}: (X \tensor Y) \tensor Z \to X \tensor (Y \tensor Z)$.
    \end{enumerate}
    Satisfying certain conditions.
\end{definition}
For a full definition, see e.g. \todo{Riehl}.
This definition surely seems daunting, and perhaps rightly so.
The point is that $\tensor$ is a way of ``combining'' the objects of the category, which should be associative, \emph{but only up to isomorphism}.
This is really all we can ask for. The prototypical example of a monoidal category is $\Set$, with $\tensor = \times$ and $I = \{*\}$.
It is not exactly true that the sets $X \times \{*\}$ and $X$ are \emph{equal} - rather, there is a natural way of identifying their elements, by throwing away the superfluous coordinate in a pair like $(x,*)$.
Similarly, the sets $(X \times Y) \times Z$ and $X \times (Y \times Z)$ are not exactly equal, but there is a natural way of identifying them, sending $((x,y),z)$ to $(x,(y,z))$.

This isomorphism means that it is safe to treat the monoidal structure as if it was really associative and unital.
The aforementioned conditions are a key part of this.
Given two products like $(X \tensor Y) \tensor (Z \tensor W)$ and $X \tensor ((Y \tensor Z) \tensor W)$, there are in fact two distinct ways of using $\alpha$ to build an isomorphism between them. If we want to treat these two objects as the same, by composing with this isomorphism, we had better ensure that these two isomorphisms are the same! If there were multiple different ways of identifying these sets with each other (supposing they are sets), we would certainly have to keep track of which one we were using.\footnotemark
Hence one of the axioms asserts that these two compositions agree - and in fact the axioms suffice to show that there is a unique identification between any two such products of the same objects, built out of $\alpha, \lambda$ and $\rho$.

\footnotetext{For a perhaps more familiar example of something like this, think of keeping track of different coordinate systems in a vector space. Even though the two maps $\bR^n \to V$ given by different bases are both isomorphisms, it is obviously still important to distinguish them. Hence we can't just replace every vector space by $\bR^n$, we must also keep track of the relationships between the possible choices of basis.}

\begin{definition}
    A \emph{symmetry} on a monoidal category is a natural isomorphism $\sigma_{A,B}: A \tensor B \to B \tensor A$ satisfying a number of compatibility axioms.
    A symmetric monoidal category is a monoidal category equipped with a symmetry.
\end{definition}
Again, for a full treatment, see \cite{Riehl}.
The point of the compatibility axioms is again that they make sure there is a unique isomorphism built from $\alpha,\lambda,\rho,\sigma$ between any pair of tensor products that ``should'' be isomorphic. Among other things, $\sigma_{A,B}\sigma_{B,A} = 1_{B \tensor A}$.
Interestingly, this particular axiom can be removed without breaking much of the theory - such a category is called a \emph{braided monoidal category}.

\begin{example}
    A monoidal category may carry multiple distinct symmetries.
    For instance, on the monoidal category $(\FdVect, \tensor, \bR)$, the maps $x \tensor y \mapsto y \tensor x$ and $x \tensor y \mapsto -y \tensor x$ are both symmetries. 
\end{example}
However, most natural examples of monoidal categories considered here are symmetric.


\begin{example}
    Consider the functor $i: \FinStoch \to Stoch$ which equips each finite set with the powerset $\sigma$-algebra, and sends a stochastic matrix to the associated Markov kernel.
    This functor is a comonoid functor.
\end{example}
\begin{example}
    Consider the category $\Gauss$ defined as follows:
    \begin{enumerate}
        \item The objects are natural numbers $n= 0, 1, \dots$.
        \item The maps $n \to m$ are given by a tuple $(A, \xi, \Sigma)$, where $M \in \bR^{n \times m}$ is a matrix, $\xi \in \bR^m$ is a vector, and $\Sigma \in \bR^{m \times m}$ is a symmetric, positive-semidefinite matrix.
        \item Composition $(B,\mu,\Gamma) \circ (A,\xi,\Sigma) = (BA, \mu + B\xi, \Gamma + B\Sigma B^T)$
    \end{enumerate}
    $\Gauss$ has a symmetric monoidal structure where $n \tensor m = n + m$, which acts on morphisms by taking direct or block sum, i.e
    \[(A,\xi,\Sigma) \tensor (B,\mu,\Gamma) = (A \oplus B, (\xi,\mu), \Sigma \oplus \Gamma)\]
where \[A \oplus B = \begin{pmatrix}A & 0 \\ 0 & B\end{pmatrix}\]
    Moreover, $\Gauss$ is a Markov category, with
\[\cop = \left(\begin{pmatrix}I_A\\I_A\end{pmatrix}, 0, 0\right)\]
    and terminal object/unit given by $0$ (We use the convention that there is a unique $n \times 0$ matrix for each $n$).
    If it wasn't clear from the chosen formulas, the interpretation of $\Gauss$ is that $n$ is $n$-dimensional space, and a map is an affine map plus (multivariate) Gaussian noise.
    This actually defines a comonoid functor $\Gauss \to \Stoch$.
    This means you can interpret a string diagram representing stochastic maps like this as either
    \begin{enumerate}
        \item A purely linear-algebra object, consisting of matrices and vectors
        \item Or a description of a probabilistic map
    \end{enumerate}
    \emph{and these interpretations are compatible}.
    \todo{Diagram with conditionals} gives you a \emph{definition} of conditional distributions in the world of $\Gauss$, which is purely an algebraic object.
    But moving this into $\Stoch$ actually gives the proper conditional distribution.
    And this doesn't need to be proven, this is just a consequence of abstract theory.
\end{example}

\begin{example}
    There's a well-known formula for the conditional distribution of a multivariate Gaussian (which in particular shows that it's Gaussian).
    Here's a sketch of an alternative way of proving this fomula:
    \begin{enumerate}
        \item Given $f: A \to X \tensor Y$ in $\Gauss$, define $f|X: A \tensor X \to Y$ by the traditional formula.
        \item Prove that this is a conditional distribution in our sense - by linear algebra.
        \item Conclude that the image in $\Stoch$ is the conditional distribution that we're trying to construct.
    \end{enumerate}
\end{example}

\begin{example}
    Suppose we have a Gaussian graphical model with the same underlying graph as the smoking example.
    For example, suppose we use a more sophisticated model where all our variables are real-valued (measuring, say, the frequency of smoking, the measured amount of tar, and the measured amount of cancer.\footnote{This is a pretty bad model - none of these variables should reasonably be negative, and the actualy distributions probably have discontinuities around zero. But we'll use it anyways.}

    As before, we have an observational distribution, which again is as depicted in figure \todo{ref}.
    This is a morphism $0 \to 3$ is $\mathsf{Gauss}$, which consists of a pair $(\xi \in \bR^3, \Sigma \in \bR^{3\time 3})$.
    Now let's find the interventional distribution under the intervention $S := U$, where $U \sim \cN(0,1)$ is a new independent normal variable.
    Repeating the argumenst of section \todo{ref}, using our formula for the conditional distributions, we obtain the following expression for the interventional distribution:
    \todo{calculate this}
\end{example}

\begin{example}
Given a measurable space $(X,\Sigma_X)$, define another measurable space $(GX, \Sigma_{GX})$ like so:
\begin{enumerate}
    \item $GX$ is the set of probability measures on $X$
    \item $\Sigma_{GX}$ is the $\sigma$-algebra generated by the maps $e_U: GX \to [0,1]$, where $e_U(P) = P(U)$,
    for each $U \in \Sigma_X$.
\end{enumerate}
Given a measurable map $f: X \to Y$, the operation of taking image (or pushforward) measure defines a measurable map $GX \to GY$.
This makes $G$ into a functor $\Meas \to \Meas$.

$G$ gives us a good description of Markov kernels, the topic that began these notes:
a Markov kernel $X \to Y$ is a measurable map $X \to GY$.

Can we describe the category $\Stoch$ in terms of $\Meas$ and $G$? It turns out we need some further information:
\begin{enumerate}
    \item To define the identities, we need a measurable map $X \to GX$. For this, we simply use the map $x \mapsto \delta_x$ sending each point to the Dirac measure.
    \item To define composition, we need to combine maps $f: X \to GY, g: Y \to GZ$ into a map $X \to GZ$.
    We can take the composition
    \[Gg \circ f : X \to GY \to GGZ,\]
    but from there, we need a way of getting back to $GZ$.
    This is provided by the ``integration'' operation, which is defined by
    \[(\mathsf{int}_A\mu)(U) = \int_{q \in GA}q(U)d\mu(q)\]
    For $\mu \in GA, A \in \Meas$
\end{enumerate}
Exercise: verify that this composition rule agrees with the one defined for $\Stoch$.
\end{example}

The structure that we have so equipped $G$ with is called the structure of a \emph{monad} - a monad is a functor $\cC \to \cC$ equipped with operations $X \to GX$ and $GGX \to GX$ for each $X \in \cC$.
The pattern of having a family of morphisms indexed by the objects of category is very common.
Typically, one must impose a certain condition to ensure this is well-behaved.

\begin{definition}
    Let $F,G: \cC \to \cD$ be functors.
    A \emph{natural transformation} $\alpha: F \to G$ is a family of maps $\{\alpha_X: FX \to GX\}_{X \in \cC}$ in $\cD$, satisfying this condition: for each map $f: X \to Y$ in $\cC$, the following diagram commutes.

    \begin{tikzcd}
        FX \ar[r, "F(f)"] \ar[d, "\alpha_X"] & FY \ar[d, "\alpha_Y"]\\
        GX \ar[r, "G(f)"] & GY
    \end{tikzcd}

\end{definition}

Exercise: verify that $x \mapsto \delta_x$ and $\int_A: GGA \to GA$ are natural transformations (between what functors?).

We can now give the definition of monad:
\begin{definition}
    A \emph{monad} on a category $\cC$ consists of
    \begin{enumerate}
        \item A functor $T: \cC \to \cC$
        \item A natural transformation $\eta: 1_\cC \to T$ called the \emph{unit}.
        \item A natural transformation $\mu: T^2 \to T$ called the \emph{multiplication}.
    \end{enumerate}
    So that the following diagrams commute for all $X \in \cC$:

    \begin{tikzcd}
        TX \ar[r, "T(\eta_X)"] \ar[d, "\eta_{TX}"] \ar[dr, "1_{TX}"] & T^2X \ar[d, "\mu"]\\
        T^2X \ar[r, "\mu"] & TX
    \end{tikzcd}

    \begin{tikzcd}
        T^3X \ar[r, "T(\mu_X)"] \ar[d, "\mu_{TX}"] & T^2X \ar[d, "\mu_X"]\\
        T^2X \ar[r, "\mu_X"] & TX
    \end{tikzcd}
\end{definition}

In fact, the construction of $\Stoch$ from $\Meas$ and $G$ is a general thing:

\begin{definition}
    Given a monad $T$ on a category $\cC$, the \emph{Kleisli category} of $T$, $Kl(T)$, is the category with
    \begin{enumerate}
        \item Objects the same as the objects of $\cC$.
        \item A map $X \to Y \in Kl(T)$ is a map $X \to TY \in Kl(T)$
        \item $1_X = \eta_X$ and the composition is defined using the multiplication.
    \end{enumerate}
\end{definition}

Exercise: verify that the Kleisli category is really a category.

\begin{example}
    We have many examples of monads.
\begin{enumerate}
    \item The functor $D: \Set \to \Set$ which sends each set to set of probability distributions on it with finite support. Exercise: What is the monad structure. Exercise: Make $Kl(D)$ into a Markov category.
    \item The functor $(-)_*: \Set \to \Set$ which sends each set $X$ to the set $X_* = X \sqcup \{*\}$, $X$ with an added basepoint. The Kleisli category can be identified with the category of sets and \emph{partial maps}.
    \item If $A$ is a partially ordered set, a monad on $A$ is a monotone map $f: A \to A$ where $a \leq f(a) = f(f(a))$.
    \item If $X$ is a topological space, and $\overline{(-)}: \cP(X) \to \cP(X)$ is the operation of taking the closure of a subset, $\overline{(-)}$ is a monad.
    \item If $A$ is a group, the functor $\Set \to \Set$ defined by $A \mapsto A \times X$ carries a monad structure. Exercise: define it.
\end{enumerate}
\end{example}

\begin{definition}
    Let $\cC$ be a symmetric monoidal category. A \emph{commutative comonoid} is an object $X$ equipped with maps $X \to I$ and $X \to X \tensor X$,
    so that the following diagrams commute.
\end{definition}
These axioms are \emph{dual} to the usual axioms of unit, commutativity and associativity - they are what results from turning the arrows around.
This definitions exhibits two patterns very common in category theory:

\begin{enumerate}
    \item \emph{Dualization} - taking a definition and ``turning the arrows around'' to get a new definition.
    \item The process of \emph{internalizing} a definition - taking an algebraic structure, usually interpreted as a set with operations, and interpreting it in any category (perhaps using some structure from the category).
\end{enumerate}

\end{document}