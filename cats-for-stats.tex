\documentclass{article}
\usepackage{eigilscmds}
\newcommand{\del}{\mathsf{del}}
\newcommand{\cop}{\mathsf{copy}}
\newcommand{\Stoch}{\mathsf{Stoch}}
\DeclareMathOperator{\ob}{ob}

\usepackage{cleveref}
\title{Category theory for Statistics}
\author{Eigil Fjeldgren Rischel}
\date{}

\begin{document}
\maketitle
\section{Introduction}

The goal of this note is to introduce certain basic concepts of category theory, in a context which will hopefuly make them understandable to people with a background in statistics or similar fields.

There are already several excellent books introducing category theory at an elementary level. I will mention, one, Riehl's \emph{Categories in Context}.
It is an excellent supplement or replacement for these notes, and available for free on the author's webpage.

The guiding philosophy of category theory is that, when trying to understand a mathematical object, it is usually extremely useful to think about the right notion of \emph{morphism}, i.e structure-preserving transformation, between this type of objects.
To this we can perhaps add another idea, that one should understand complicated things by understanding how they are composed of simpler things.

I will try to describe several ways to bring these viewpoints to bear on questions of statistics. First, we will take \emph{Markov kernels} as the notion of transformation between measurable spaces, and try to do statistics in this context.
An interesting fact about this approach is that many definitions and results in statistics fall out of the ``abstract structure'' of this setting, and are thus in a sense independent of measure theory.

I will generally skip over technical details about measure theory without much comment or reference. \todo{flesh}
\section{Markov kernels}
Recall the definition of a \emph{Markov kernel}
\begin{definition}
    If $(A,\Sigma), (B,\Gamma)$ are measurable spaces, a \emph{Markov kernel} $A \to B$ is given by a map $f: A \times \Gamma \to [0,1]$ such that
    \begin{itemize}
        \item For each $a\in A$, $f(a,-)$ is a probability measure on $B$.
        \item For each $U \in \Gamma$, $f(-,U): A \to [0,1]$ is a measurable function.
    \end{itemize}
\end{definition}
Markov kernels can be conceptualized as ``maps with randomness'', or perhaps as conditional probability distributions.

\begin{definition}
Given markov kernels $A \labelto{f} B \labelto{g} C$, we can form their \emph{composition} $g\circ f: A \to C$.
Is is given by $(g\circ f)(a)(U) = \bE_{b \sim f(a)}g(b)(U)$.
In other words, it is the average probability, according to $g(b)$, of $U$, if $b$ is distributed according to $f(a)$.
\end{definition}
(Of course, it must be verified that this is a Markov kernel)

If we think of a Markov kernel as a map involving some randomness, this composition simply gives the composite map, under the assumption that the two component maps use independent sources of randomness.
If we instead think of a Markov kernel as a conditional distribution, then the composition is given by computing the composed conditional distribution under the assumption of conditional independence.
If $X, Y, Z$ are stochastic variables with values in $A,B,C$, so that $f$ is the conditional distribution of $Y$ given $X$, and $g$ is the conditional distribution of $Z$ given $Y$, then $g\circ f$ is the conditional distribution of $Z$ given $X$, if and only if $Z$ is conditionally independent of $X$ given $Y$. (We will give a neat proof of this later)
A few more constructions are important: given $f:A \to B$ and $g:X \to Y$ Markov kernels, we can form the ``parallel composition'' (we might call the previous type "sequential" composition) $A \times X \to B \times Y$,
given essentially by $P(b,y|a,x) = P(b|a)P(y|x)$. We write this as $f \tensor g$. To harmonize the notation, we will also write the product measure space as $A \tensor B$, rather than $A \times B$.


We can depict Markov kernels in diagrams like this:

\todo{examples of diagrams}

The meaning of these is probably quite intuitive, but let us spell it out

\begin{enumerate}
    \item A line represents a measurable space. A collection of lines drawn next to each other represent the product of those spaces. 
    \item Each labeled box represents a Markov kernel.
    \item The diagram obtained by putting Markov kernels on top of each other and wiring them together represents the (sequential) composition.
    \item The diagram obtained by putting kernels next to each other represents the parallel composition.\footnotemark
\end{enumerate}

\footnote{There is an issue here regarding associativity - because the spaces $(A \tensor B) \tensor C$ and $A \tensor (B \tensor C)$ are not \emph{equal}, there is a question of which order to apply the parallel composition in. This turns out to not really matter - we can abuse notation and identify these two sets without worrying about it}

Using this, we can break down a diagram like (ref)\todo{fix} into $(f \tensor g) \circ h$.
To break down (ref)\todo{fix - one with identities} we need to add the convention that an undecorated line stands for the \emph{identity kernel} $X \to X$ which sends $x\in X$ to the dirac measure $\delta_x$.
This choice is not arbitrary - to make sure that each diagram represents the same kernel, no matter how we break it down, the kernel $f_X$ represented by the line needs to satisfy $f_Yg = g = gf_X$ for all kernels $g$.
It's clear that any kernel which satisfies this is equal to the identity kernel.
We write the identity kernel $1_X$.

To break down (ref)\todo{fix - one with crossing wires}, we need to add another convention, namely that a pair of crossed wires represents the kernel $X \tensor Y \to Y \tensor X$
which sends $(x,y)$ to the dirac measure $\delta_{(y,x)}$.
If we need notation for this kernel, we write it as $\sigma_{X,Y}$.

Observe that a kernel which sends each point to a dirac measure is essentially the same thing as a (measurable) function, viewed as a stochastic process which is not really stochastic at all, but deterministic. In this sense the identity kernel ``is just'' the identity function, and $\sigma_{X,Y}$ ``is'' the map swapping the coordinates.

Let us introduce a few more pieces of special notation.

\begin{enumerate}
    \item For each $X$ there is a kernel $X \to X \tensor X$ sending $x \in X$ to the dirac measure $\delta_{(x,x)}$.
    We write this as $\mathsf{copy}_X: X \to X \tensor X$ and draw it as a dot, like\todo{figure}. This corresponds to the diagonal map $X \to X \tensor X$ in the sense discussed above.
    \item Also for each $X$, there is a unique kernel $X \to 1$, where $1$ is the one-point space (since there is only one probability measure on this space).
    We write this as $\mathsf{del}_X: X \to 1$, and draw it as a dot with nothing coming out of it, like\todo{figure}. This also corresponds to the unique deterministic map $X \to 1$.
\end{enumerate}

To facilitate (2), we need to add the convention that the lack of a string can represent $1$.
This essentially means we're using the identification $X \times 1 = X$ - just like the previous footnote about associativity, this is harmless.
A kernel $1 \to X$ is just the same thing as a probability distribution on $X$. We draw these as triangles, like\todo{figure}.

We now have a rich graphical language for drawing Markov kernels.
It turns out that a lot of questions that are important in statistics can be phrased as questions about diagrams.
\begin{example}
    Consider two random variables $X\in A,Y\in B$with join distribution $P: 1 \to A \tensor B$. Then:
    \begin{enumerate}
        \item $X$ is independent of $Y$ if and only if these two diagrams represent the same kernel.
        \item $f:A \to B$ is the (a) conditional probability of $Y$ given $X$ if and only if these two diagrams represent the same kernel.
    \end{enumerate}
    We can combine these:
    \begin{enumerate}
        \item[3] Given three random variables with joint distribution $P: 1 \to X \tensor Y \tensor Z$, $X$ and $Y$ are conditionally independent given $Z$ if and only if there is a diagram of the depicted form equal to the first one. \todo{awkward formulation}.
        To see this, first note that a diagram of this form\todo{new diagram} equal to the original one gives a conditional distribution of $X,Y$ given $Z$.
        Then they are independent if and only if this is a product distribution for ($P$-almost) all $z\in C$, if and only if the depicted equality holds.
    \end{enumerate}
\label{ex:indeps}
\end{example}
\begin{example}
    Suppose we have $P: I \to (A \tensor B \tensor C)$ representing the joint distribution of three random variables $(X,Y,Z)$.
    Suppose $P$ factors like this:
    \todo{diagram involving $f: A \to B$ and $g: B \to C$}
    Then $X$ is independent of $Z$ given $Y$.
    To see this, consider this diagram.
    Since the conditional probability of $X$ given $Y$ exists, we can rewrite to this diagram:
    Which is precisely what we wanted, by \cref{ex:indeps} above.
\end{example}

\todo{some worked examples}

\section{Abstract structure}
Hopefully by now I've given you some idea that these diagrams are a nice way of working with random variables.
Now I'm going to pull the rug out and reveal that we've secretly been working with category theory the whole time.
The structure of the diagrams we have been working with corresponds exactly to the structure of a certain type of category,
known by several names in the literature. I am going to call them Markov categories, a name introduced by Tobias Fritz in \todo{ref!}.
It's a great reference, which I have largely been cribbing for these notes so far, although it may not be very readable without a background in category theory.

Let us ask this question: if we wanted to make sense of the diagrams we have been using so far, but we didn't know anything about the underlying objects - didn't know that the labels on the lines correspond to measurable spaces, or that the boxes correspond to kernels, or anything like that - what is the bare minimum of structure we need to intepret these diagrams?
This may seem like an extremely odd question, but in fact it is extremely natural from the point of view of category theory.

\begin{enumerate}
    \item Certainly there is a set of possible labels for the lines (e.g. measurable spaces)\footnotemark, which we could maybe just call the set of \emph{labels}.
    \item There is also a set of possible ``connections'', e.g. kernels, which are the things that each diagram can be interpreted as. Let us just call these the set of \emph{diagrams} (although there is a subtle issue with this - graphically distinct diagrams may depict the same kernel).
    \item Each diagram has a certain collection of labels as its beginning, and another one as its end. We can write this symbolically for now as
    \[f: x_1, x_2, \dots x_m \to y_1, y_2, \dots, y_n\]
    \item If the labels at the beginning of a diagram agree with the labels at the end of another diagram, we can put the diagrams on top of one another, \emph{composing} them.
    Write this as functional composition, i.e:
    \[f: x_1, x_2, \dots x_m \to y_1, y_2, \dots y_n, g:y_1, \dots y_n \to z_1, \dots z_k\]
    \[g\circ f : x_1, \dots x_m \to z_1, \dots z_k\]
    \item We can also put diagrams next to each other (no matter what the labels are) - in that case, we also concatenate the lists of labels. Write this using $\tensor$.
    \item For each label, there are special diagrams $\cop_X: X \to X,X$ and $\del_X: X \to$ (meaning $\del$ has an \emph{empty} list of output labels).
    \item There is a special diagram $1_X : X \to X$, and a special diagrams $\sigma_{X,Y}: X,Y \to Y,X$.
\end{enumerate}
\footnotemark{Of course, there is not really a \emph{set} of measurable spaces, since this would entail a set of all sets.. we can ignore this issue for now, or suppose that for the first part we worked only with a restricted class of measurable spaces.}

The structure sketched above corresponds roughly to what category theorists call a \emph{colored PROP} (although of course I have not been completely precise).
The only exception is 5., which turns out to be someone special for probability theory. We will meet it again later.
In fact, we have a bit of extra structure, which we have not really made visible in the diagrams yet (and which in fact doesn't really have a consistent notation).
This is because kernels $A,B \to C$ are actually kernels $A \tensor B \to C$ - diagrams can always be considered as having a single source and target.
Thus we can add these bits of structure:
\begin{enumerate}
    \item For each pair of labels $x,y$, there is a diagram $f: x,y \to x\tensor y$ with the following property: composing with $f$ gives a bijection between diagrams $x \tensor y \to z_1, \dots z_n$ and diagrams $x,y \to z_1, \dots z_n$, for all sequences $z_1, \dots z_n$ of labels.
    \item Ther is a label $I$ and a diagram $f: \to I$ with the following property: composing with $f$ gives a bijection between diagrams $I \to z_1, \dots z_n$ and diagrams $\to z_1, \dots z_n$, for all sequences of labels.
\end{enumerate}
This makes our structure into a \emph{representable} colored PROP.
With this, we don't actually need to discuss the maps with multiple inputs and outputs - they are completely described by describing single input/output maps.
This is in many ways more natural - we don't usually speak of a map with two inputs $A, B \to C$, but just of a map $A \times B \to C$, which we may think of as having one or two inputs.
In the history of category theory, that's how it went, too. People first developed the notion of \emph{monoidal} category that I'm about to describe, and only later came up with representable PROPs (first, of course, they came up with PROPs, which are interesting in their own right).
The reason I developed things in this rather silly order is because string diagrams are more naturally interpreted in PROPs.


I will give the next definition in a number of steps, corresponding very roughly to the order in which things were introduced above.
\begin{definition}
    A \emph{category} $\cC$ consists of the following data.
    \begin{enumerate}
        \item A set of \emph{objects}, $\ob \cC$.
        \item For each $X,Y \in \ob \cC$, a set $\cC(X,Y)$ of \emph{morphisms $X \to Y$}. If $f\in \cC(X,Y)$ we write $f: X \to Y$.
        \item For each $X,Y,Z \in \ob \cC$, a \emph{composition} $\cC(Y,Z) \times \cC(X,Y) \to \cC(X,Z)$
        \item For each $X \in \ob \cC$, an \emph{identity} $1_X \in \cC(X,X)$.
    \end{enumerate}
    Satisfying $1_X \circ f = f = 1_Y \circ f$ for $f: X \to Y$, and $(f\circ g)\circ h = f \circ (g \circ h)$.
\end{definition}
One usually simply writes $X \in \cC$ when $X$ is an object of $\cC$, for brevity.
\begin{example}
    Some examples of categories are
    \begin{enumerate}
        \item The category $\Set$ where objects are sets, morphisms are functions, and composition is composition of functions.
        \item The category $\Meas$ where objects are measurable spaces, morphisms are measurable functions, and composition is again simply composition of functions.
        \item The category $\Stoch$ where objects are measurable spaces, morphisms are \emph{Markov kernels}, and composition is as above.
    \end{enumerate}
\end{example}
These examples illustrate a common problem: a category is not uniquely determined by its objects, so $f:X \to Y$ is ambiguous - if $X,Y$ are measurable spaces, $f$ could be a Markov kernel or a measurable map. This is usually taken to be clear from context, or one can write $f:X \to Y \in \Stoch$ for clarity.
Also, most classes of objects do have a ``natural'' notion of morphisms, so that $\Meas$ may be called ``the category of measurable spaces'' without too much worry.
\end{document}